<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural Networks | Hao Meng - Academic Homepage</title><link>/tag/neural-networks/</link><atom:link href="/tag/neural-networks/index.xml" rel="self" type="application/rss+xml"/><description>Neural Networks</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2025 Hao Meng. All rights reserved.</copyright><lastBuildDate>Mon, 01 Jan 2024 00:00:00 +0000</lastBuildDate><image><url>/images/icon_hu_a9296fd8e02a9164.png</url><title>Neural Networks</title><link>/tag/neural-networks/</link></image><item><title>Advanced Deep Learning for Computer Vision Applications</title><link>/publication/deep-learning-paper/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>/publication/deep-learning-paper/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Recent advances in deep learning have revolutionized computer vision applications. However, existing architectures often struggle with computational efficiency and generalization across diverse datasets. In this work, we propose a novel hybrid architecture that combines the strengths of convolutional neural networks with attention mechanisms to address these limitations.&lt;/p>
&lt;p>Our approach introduces several key innovations:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Multi-scale Feature Fusion&lt;/strong>: We develop a novel feature fusion strategy that effectively combines information from multiple scales&lt;/li>
&lt;li>&lt;strong>Adaptive Attention Mechanism&lt;/strong>: Our attention module dynamically adjusts its focus based on input characteristics&lt;/li>
&lt;li>&lt;strong>Efficient Training Strategy&lt;/strong>: We propose a curriculum learning approach that significantly reduces training time&lt;/li>
&lt;/ol>
&lt;h2 id="methodology">Methodology&lt;/h2>
&lt;h3 id="architecture-overview">Architecture Overview&lt;/h3>
&lt;p>Our proposed architecture consists of three main components:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Backbone Network&lt;/strong>: A modified ResNet-50 that serves as the feature extractor&lt;/li>
&lt;li>&lt;strong>Attention Module&lt;/strong>: A novel attention mechanism that learns to focus on relevant image regions&lt;/li>
&lt;li>&lt;strong>Classification Head&lt;/strong>: A lightweight classifier that produces final predictions&lt;/li>
&lt;/ol>
&lt;h3 id="mathematical-formulation">Mathematical Formulation&lt;/h3>
&lt;p>The attention mechanism is formulated as:&lt;/p>
&lt;p>$$A(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$&lt;/p>
&lt;p>where $Q$, $K$, and $V$ represent query, key, and value matrices respectively.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>We evaluate our method on three benchmark datasets:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ImageNet&lt;/strong>: 1.2M training images, 50K validation images&lt;/li>
&lt;li>&lt;strong>CIFAR-100&lt;/strong>: 50K training images, 10K test images&lt;/li>
&lt;li>&lt;strong>Places365&lt;/strong>: 1.8M training images, 36.5K validation images&lt;/li>
&lt;/ul>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>Our method achieves state-of-the-art performance across all datasets:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>Top-1 Accuracy&lt;/th>
&lt;th>Top-5 Accuracy&lt;/th>
&lt;th>Parameters&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ImageNet&lt;/td>
&lt;td>95.2%&lt;/td>
&lt;td>98.1%&lt;/td>
&lt;td>23M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CIFAR-100&lt;/td>
&lt;td>89.7%&lt;/td>
&lt;td>98.9%&lt;/td>
&lt;td>23M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Places365&lt;/td>
&lt;td>87.3%&lt;/td>
&lt;td>97.5%&lt;/td>
&lt;td>23M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="ablation-studies">Ablation Studies&lt;/h3>
&lt;p>We conduct extensive ablation studies to validate each component:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Attention Mechanism&lt;/strong>: Removing the attention module reduces accuracy by 3.2%&lt;/li>
&lt;li>&lt;strong>Multi-scale Fusion&lt;/strong>: Without feature fusion, performance drops by 2.1%&lt;/li>
&lt;li>&lt;strong>Curriculum Learning&lt;/strong>: Standard training increases training time by 40%&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;h3 id="key-contributions">Key Contributions&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Novel Architecture&lt;/strong>: Our hybrid approach effectively combines CNN and attention mechanisms&lt;/li>
&lt;li>&lt;strong>Efficiency&lt;/strong>: The proposed method is 60% faster than baseline approaches&lt;/li>
&lt;li>&lt;strong>Generalization&lt;/strong>: Strong performance across diverse datasets demonstrates robust generalization&lt;/li>
&lt;/ol>
&lt;h3 id="limitations">Limitations&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Computational Cost&lt;/strong>: The attention mechanism adds computational overhead&lt;/li>
&lt;li>&lt;strong>Memory Usage&lt;/strong>: Higher memory requirements compared to standard CNNs&lt;/li>
&lt;li>&lt;strong>Training Complexity&lt;/strong>: More complex training procedure requires careful hyperparameter tuning&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We present a novel deep learning architecture that effectively combines convolutional neural networks with attention mechanisms. Our method achieves state-of-the-art performance while maintaining computational efficiency. Future work will focus on reducing computational complexity and extending the approach to video understanding tasks.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We thank the anonymous reviewers for their valuable feedback. This work was supported by NSF Grant #1234567 and industry partnership with TechCorp Inc.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ol>
&lt;li>He, K., et al. &amp;ldquo;Deep residual learning for image recognition.&amp;rdquo; CVPR 2016.&lt;/li>
&lt;li>Vaswani, A., et al. &amp;ldquo;Attention is all you need.&amp;rdquo; NeurIPS 2017.&lt;/li>
&lt;li>Dosovitskiy, A., et al. &amp;ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.&amp;rdquo; ICLR 2021.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>&lt;em>This is an example publication. Please replace with your actual research papers and publications.&lt;/em>&lt;/p></description></item></channel></rss>