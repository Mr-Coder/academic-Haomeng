<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Publications | Hao Meng - Academic Homepage</title><link>/publication/</link><atom:link href="/publication/index.xml" rel="self" type="application/rss+xml"/><description>Publications</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2025 Hao Meng. All rights reserved.</copyright><image><url>/images/icon_hu_a9296fd8e02a9164.png</url><title>Publications</title><link>/publication/</link></image><item><title>Advanced Deep Learning for Computer Vision Applications</title><link>/publication/deep-learning-paper/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>/publication/deep-learning-paper/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Recent advances in deep learning have revolutionized computer vision applications. However, existing architectures often struggle with computational efficiency and generalization across diverse datasets. In this work, we propose a novel hybrid architecture that combines the strengths of convolutional neural networks with attention mechanisms to address these limitations.&lt;/p>
&lt;p>Our approach introduces several key innovations:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Multi-scale Feature Fusion&lt;/strong>: We develop a novel feature fusion strategy that effectively combines information from multiple scales&lt;/li>
&lt;li>&lt;strong>Adaptive Attention Mechanism&lt;/strong>: Our attention module dynamically adjusts its focus based on input characteristics&lt;/li>
&lt;li>&lt;strong>Efficient Training Strategy&lt;/strong>: We propose a curriculum learning approach that significantly reduces training time&lt;/li>
&lt;/ol>
&lt;h2 id="methodology">Methodology&lt;/h2>
&lt;h3 id="architecture-overview">Architecture Overview&lt;/h3>
&lt;p>Our proposed architecture consists of three main components:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Backbone Network&lt;/strong>: A modified ResNet-50 that serves as the feature extractor&lt;/li>
&lt;li>&lt;strong>Attention Module&lt;/strong>: A novel attention mechanism that learns to focus on relevant image regions&lt;/li>
&lt;li>&lt;strong>Classification Head&lt;/strong>: A lightweight classifier that produces final predictions&lt;/li>
&lt;/ol>
&lt;h3 id="mathematical-formulation">Mathematical Formulation&lt;/h3>
&lt;p>The attention mechanism is formulated as:&lt;/p>
&lt;p>$$A(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$&lt;/p>
&lt;p>where $Q$, $K$, and $V$ represent query, key, and value matrices respectively.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>We evaluate our method on three benchmark datasets:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ImageNet&lt;/strong>: 1.2M training images, 50K validation images&lt;/li>
&lt;li>&lt;strong>CIFAR-100&lt;/strong>: 50K training images, 10K test images&lt;/li>
&lt;li>&lt;strong>Places365&lt;/strong>: 1.8M training images, 36.5K validation images&lt;/li>
&lt;/ul>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>Our method achieves state-of-the-art performance across all datasets:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>Top-1 Accuracy&lt;/th>
&lt;th>Top-5 Accuracy&lt;/th>
&lt;th>Parameters&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ImageNet&lt;/td>
&lt;td>95.2%&lt;/td>
&lt;td>98.1%&lt;/td>
&lt;td>23M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CIFAR-100&lt;/td>
&lt;td>89.7%&lt;/td>
&lt;td>98.9%&lt;/td>
&lt;td>23M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Places365&lt;/td>
&lt;td>87.3%&lt;/td>
&lt;td>97.5%&lt;/td>
&lt;td>23M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="ablation-studies">Ablation Studies&lt;/h3>
&lt;p>We conduct extensive ablation studies to validate each component:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Attention Mechanism&lt;/strong>: Removing the attention module reduces accuracy by 3.2%&lt;/li>
&lt;li>&lt;strong>Multi-scale Fusion&lt;/strong>: Without feature fusion, performance drops by 2.1%&lt;/li>
&lt;li>&lt;strong>Curriculum Learning&lt;/strong>: Standard training increases training time by 40%&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;h3 id="key-contributions">Key Contributions&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Novel Architecture&lt;/strong>: Our hybrid approach effectively combines CNN and attention mechanisms&lt;/li>
&lt;li>&lt;strong>Efficiency&lt;/strong>: The proposed method is 60% faster than baseline approaches&lt;/li>
&lt;li>&lt;strong>Generalization&lt;/strong>: Strong performance across diverse datasets demonstrates robust generalization&lt;/li>
&lt;/ol>
&lt;h3 id="limitations">Limitations&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Computational Cost&lt;/strong>: The attention mechanism adds computational overhead&lt;/li>
&lt;li>&lt;strong>Memory Usage&lt;/strong>: Higher memory requirements compared to standard CNNs&lt;/li>
&lt;li>&lt;strong>Training Complexity&lt;/strong>: More complex training procedure requires careful hyperparameter tuning&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We present a novel deep learning architecture that effectively combines convolutional neural networks with attention mechanisms. Our method achieves state-of-the-art performance while maintaining computational efficiency. Future work will focus on reducing computational complexity and extending the approach to video understanding tasks.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We thank the anonymous reviewers for their valuable feedback. This work was supported by NSF Grant #1234567 and industry partnership with TechCorp Inc.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ol>
&lt;li>He, K., et al. &amp;ldquo;Deep residual learning for image recognition.&amp;rdquo; CVPR 2016.&lt;/li>
&lt;li>Vaswani, A., et al. &amp;ldquo;Attention is all you need.&amp;rdquo; NeurIPS 2017.&lt;/li>
&lt;li>Dosovitskiy, A., et al. &amp;ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.&amp;rdquo; ICLR 2021.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>&lt;em>This is an example publication. Please replace with your actual research papers and publications.&lt;/em>&lt;/p></description></item><item><title>Qualia role-based quantity relation extraction for solving algebra story problems</title><link>/publication/qualia-role-quantity-relation/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>/publication/qualia-role-quantity-relation/</guid><description>&lt;p>This paper presents a novel approach to solving algebra story problems in Chinese using a qualia role-based entity-dependency graph (EDG). The method addresses the limitations of traditional neural solvers by explicitly modeling quantity relations hidden in the qualia roles of mathematical objects.&lt;/p>
&lt;h2 id="key-contributions">Key Contributions&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Entity-Dependency Graph (EDG)&lt;/strong>: A novel representation that captures quantity relations in algebra story problems&lt;/li>
&lt;li>&lt;strong>Qualia Role Integration&lt;/strong>: Leverages qualia structure to understand implicit relationships between mathematical entities&lt;/li>
&lt;li>&lt;strong>Algorithm Design&lt;/strong>: Develops specific algorithms for EDG generation and quantity relation extraction&lt;/li>
&lt;li>&lt;strong>Chinese Language Support&lt;/strong>: Addresses the unique challenges of solving algebra problems stated in Chinese&lt;/li>
&lt;/ol>
&lt;h2 id="methodology">Methodology&lt;/h2>
&lt;p>The proposed approach consists of several key components:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Problem Text Analysis&lt;/strong>: Natural language processing to understand the problem statement&lt;/li>
&lt;li>&lt;strong>Entity Extraction&lt;/strong>: Identification of mathematical objects and their properties&lt;/li>
&lt;li>&lt;strong>Qualia Role Assignment&lt;/strong>: Mapping entities to their qualia roles (formal, constitutive, telic, agentive)&lt;/li>
&lt;li>&lt;strong>EDG Construction&lt;/strong>: Building the entity-dependency graph based on qualia relationships&lt;/li>
&lt;li>&lt;strong>Quantity Relation Extraction&lt;/strong>: Deriving mathematical relationships from the EDG&lt;/li>
&lt;li>&lt;strong>Solution Generation&lt;/strong>: Converting extracted relations into mathematical expressions&lt;/li>
&lt;/ul>
&lt;h2 id="experimental-results">Experimental Results&lt;/h2>
&lt;p>The experimental evaluation demonstrates the effectiveness of the proposed method in solving Chinese algebra story problems, showing improvements over traditional end-to-end neural approaches in terms of accuracy and interpretability.&lt;/p>
&lt;h2 id="impact">Impact&lt;/h2>
&lt;p>This work contributes to the field of educational technology by providing a more interpretable and effective approach to automated math problem solving, with potential applications in intelligent tutoring systems and educational AI.&lt;/p></description></item><item><title>Prompt-based missing entity recovery for solving arithmetic word problems</title><link>/publication/prompt-based-missing-entity/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>/publication/prompt-based-missing-entity/</guid><description>&lt;p>This paper addresses the challenge of missing entity recovery in arithmetic word problems using prompt-based techniques. The approach leverages natural language prompts to guide the system in identifying and recovering missing mathematical entities that are crucial for problem solving.&lt;/p>
&lt;h2 id="key-contributions">Key Contributions&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Prompt-based Framework&lt;/strong>: Novel use of prompts for entity recovery in mathematical problem solving&lt;/li>
&lt;li>&lt;strong>Missing Entity Detection&lt;/strong>: Systematic approach to identify missing entities in word problems&lt;/li>
&lt;li>&lt;strong>Recovery Strategies&lt;/strong>: Effective methods for reconstructing missing information&lt;/li>
&lt;li>&lt;strong>Performance Improvement&lt;/strong>: Enhanced accuracy in arithmetic word problem solving&lt;/li>
&lt;/ol>
&lt;h2 id="methodology">Methodology&lt;/h2>
&lt;p>The proposed approach includes:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Entity Analysis&lt;/strong>: Comprehensive analysis of entities in word problems&lt;/li>
&lt;li>&lt;strong>Missing Entity Identification&lt;/strong>: Detection of implicit or missing entities&lt;/li>
&lt;li>&lt;strong>Prompt Design&lt;/strong>: Crafting effective prompts for entity recovery&lt;/li>
&lt;li>&lt;strong>Recovery Process&lt;/strong>: Systematic recovery of missing entities&lt;/li>
&lt;li>&lt;strong>Integration&lt;/strong>: Seamless integration with existing problem solving systems&lt;/li>
&lt;/ul>
&lt;p>This work contributes to the advancement of automated mathematical problem solving by addressing a key challenge in natural language understanding for educational applications.&lt;/p></description></item><item><title>A bi-channel math word problem solver with understanding and reasoning</title><link>/publication/bi-channel-solver/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>/publication/bi-channel-solver/</guid><description>&lt;p>This paper introduces a novel bi-channel architecture for solving mathematical word problems that separates understanding and reasoning processes. The system leverages two distinct channels to handle different aspects of problem solving, leading to improved performance and interpretability.&lt;/p>
&lt;h2 id="key-contributions">Key Contributions&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Bi-channel Architecture&lt;/strong>: Novel dual-channel approach for math problem solving&lt;/li>
&lt;li>&lt;strong>Understanding Channel&lt;/strong>: Dedicated channel for problem comprehension&lt;/li>
&lt;li>&lt;strong>Reasoning Channel&lt;/strong>: Specialized channel for mathematical reasoning&lt;/li>
&lt;li>&lt;strong>Integration Strategy&lt;/strong>: Effective combination of understanding and reasoning outputs&lt;/li>
&lt;li>&lt;strong>Performance Enhancement&lt;/strong>: Improved accuracy and interpretability&lt;/li>
&lt;/ol>
&lt;h2 id="methodology">Methodology&lt;/h2>
&lt;p>The proposed system consists of:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Understanding Channel&lt;/strong>: Processes natural language to extract mathematical concepts&lt;/li>
&lt;li>&lt;strong>Reasoning Channel&lt;/strong>: Performs mathematical reasoning and computation&lt;/li>
&lt;li>&lt;strong>Feature Extraction&lt;/strong>: Comprehensive feature extraction from problem text&lt;/li>
&lt;li>&lt;strong>Channel Integration&lt;/strong>: Fusion of understanding and reasoning outputs&lt;/li>
&lt;li>&lt;strong>Solution Generation&lt;/strong>: Final solution synthesis from both channels&lt;/li>
&lt;/ul>
&lt;h2 id="experimental-results">Experimental Results&lt;/h2>
&lt;p>The bi-channel approach demonstrates superior performance compared to single-channel baselines, showing the effectiveness of separating understanding and reasoning processes in mathematical problem solving.&lt;/p>
&lt;p>This work contributes to the field of educational AI by providing a more structured and interpretable approach to automated mathematical problem solving.&lt;/p></description></item><item><title>The context-oriented system based on ELECTRA for solving math word problem</title><link>/publication/context-oriented-electra/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>/publication/context-oriented-electra/</guid><description>&lt;p>This paper introduces a context-oriented system that utilizes ELECTRA, a pre-trained language model, to enhance the understanding and solving of mathematical word problems. The system focuses on capturing contextual information to improve problem comprehension and solution accuracy.&lt;/p>
&lt;h2 id="key-contributions">Key Contributions&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>ELECTRA Integration&lt;/strong>: Effective use of ELECTRA for mathematical problem understanding&lt;/li>
&lt;li>&lt;strong>Context-Oriented Approach&lt;/strong>: Emphasis on contextual information for better problem comprehension&lt;/li>
&lt;li>&lt;strong>System Architecture&lt;/strong>: Comprehensive system design for math word problem solving&lt;/li>
&lt;li>&lt;strong>Performance Evaluation&lt;/strong>: Thorough evaluation of the proposed approach&lt;/li>
&lt;/ol>
&lt;h2 id="methodology">Methodology&lt;/h2>
&lt;p>The proposed system includes:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Context Analysis&lt;/strong>: Deep understanding of problem context using ELECTRA&lt;/li>
&lt;li>&lt;strong>Semantic Processing&lt;/strong>: Enhanced semantic understanding of mathematical concepts&lt;/li>
&lt;li>&lt;strong>Problem Decomposition&lt;/strong>: Breaking down complex problems into manageable components&lt;/li>
&lt;li>&lt;strong>Solution Generation&lt;/strong>: Systematic approach to generating mathematical solutions&lt;/li>
&lt;li>&lt;strong>Evaluation Framework&lt;/strong>: Comprehensive evaluation of system performance&lt;/li>
&lt;/ul>
&lt;p>This work demonstrates the effectiveness of transformer-based language models in educational applications, particularly in the domain of automated mathematical problem solving.&lt;/p></description></item></channel></rss>